# coding: utf-8
# Copyright (c) Pymatgen Development Team.
# Distributed under the terms of the MIT License.

from __future__ import division, print_function, unicode_literals, absolute_import

"""
This module implements classes for processing Lammps output files:

    1. log file: contains the thermodynamic data with the format set by the
        'thermo_style' command

    2. trajectory file(dump file): the file generated by the 'dump' command

    Restrictions:
        The first 2 fields of the ATOMS section in the trajectory(dump) file
        must be the atom id and the atom type. There can be arbitrary number
        of fields after that and they all will be treated as floats and
        updated based on the field names in the ITEM: ATOMS line.
"""

import re
import os
import glob
from io import open, StringIO

import numpy as np

from monty.json import MSONable
from monty.io import zopen

from pymatgen.core.periodic_table import _pt_data
from pymatgen.core.structure import Structure
from pymatgen.core.lattice import Lattice
from pymatgen.analysis.diffusion_analyzer import DiffusionAnalyzer
from pymatgen.io.lammps.data import LammpsBox, LammpsData

__author__ = "Kiran Mathew"
__email__ = "kmathew@lbl.gov"
__credits__ = "Navnidhi Rajput, Michael Humbert"


# TODO write parser for one and multi thermo_styles
class LammpsLog(MSONable):
    """
    Parser for LAMMPS log file.
    """

    def __init__(self, log_file="log.lammps"):
        """
        Args:
            log_file (string): path to the log file
        """
        self.log_file = os.path.abspath(log_file)
        self.timestep = -1
        self._parse_log()

    def _parse_log(self):
        """
        Parse the log file for run and thermodynamic data.
        Sets the thermodynamic data as a structured numpy array with field names
        taken from the custom thermo_style command. thermo_style one and multi
        are not supported yet
        """

        thermo_data = []
        fixes = []
        d_build = None
        thermo_pattern = None
        with open(self.log_file, 'r') as logfile:
            for line in logfile:
                # timestep, the unit depedns on the 'units' command
                time = re.search(r'timestep\s+([0-9]+)', line)
                if time and not d_build:
                    self.timestep = float(time.group(1))
                # total number md steps
                steps = re.search(r'run\s+([0-9]+)', line)
                if steps and not d_build:
                    self.nmdsteps = int(steps.group(1))
                # simulation info
                fix = re.search(r'fix.+', line)
                if fix and not d_build:
                    fixes.append(fix.group())
                # dangerous builds
                danger = re.search(r'Dangerous builds\s+([0-9]+)', line)
                if danger and not d_build:
                    d_build = int(steps.group(1))
                # logging interval
                thermo = re.search(r'thermo\s+([0-9]+)', line)
                if thermo and not d_build:
                    self.interval = float(thermo.group(1))
                # thermodynamic data, set by the thermo_style command
                fmt = re.search(r'thermo_style.+', line)
                if fmt and not d_build:
                    thermo_type = fmt.group().split()[1]
                    fields = fmt.group().split()[2:]
                    no_parse = ["one", "multi"]
                    if thermo_type in no_parse:
                        thermo_data.append("cannot parse thermo_style")
                    else:
                        thermo_pattern_string = r"\s*([0-9eE\.+-]+)" + "".join(
                            [r"\s+([0-9eE\.+-]+)" for _ in range(len(fields) - 1)])
                        thermo_pattern = re.compile(thermo_pattern_string)
                if thermo_pattern:
                    if thermo_pattern.search(line):
                        m = thermo_pattern.search(line)
                        thermo_data.append(tuple([float(x) for x in m.groups()]))

        if thermo_data:
            if isinstance(thermo_data[0], str):
                self.thermo_data = [thermo_data]
            else:
                # numpy arrays are easier to reshape, previously we used np.array with dtypes
                self.thermo_data = {
                    fields[i]: [thermo_data[j][i] for j in range(len(thermo_data))]
                    for i in range(len(fields))}

        self.fixes = fixes
        self.dangerous_builds = d_build

    def as_dict(self):
        d = {}
        for attrib in [a for a in dir(self)
                       if not a.startswith('__') and not callable(getattr(self, a))]:
            d[attrib] = getattr(self, attrib)
        d["@module"] = self.__class__.__module__
        d["@class"] = self.__class__.__name__
        return d

    # not really needed ?
    @classmethod
    def from_dict(cls, d):
        return cls(log_file=d["log_file"])


class LammpsDump(MSONable):
    """
    Dump file parser.

    .. attribute:: steps

        All steps in the dump as a list of
        {"timestep": current timestep,
         "natoms": no. of atoms,
         "box": simulation box (optional),
         "atoms_data": dumped data for atoms as 2D np.array}

    .. attribute:: timesteps

        List of timesteps in sequence.

    """

    def __init__(self, filename, parse_box=True, dtype=float):
        """

        Args:
            filename (str): Filename to parse. The timestep wildcard
                ('*') is supported and the files are parsed in the
                sequence of timestep.
            parse_box (bool): Whether parse box info for each step.
                Default to True.
            dtype: np.dtype for atoms data array.

        """
        self.filename = filename
        self.parse_box = parse_box
        self.dtype = dtype

        fnames = glob.glob(self.filename)
        if len(fnames) > 1:
            pattern = r"%s" % filename.replace("*", "([0-9]+)")
            pattern = pattern.replace("\\", "\\\\")
            fnames = sorted(fnames,
                            key=lambda f: int(re.match(pattern, f).group(1)))
        steps = []
        for fname in fnames:
            with zopen(fname, "rt") as f:
                run = f.read()
            dumps = run.split("ITEM: TIMESTEP")[1:]
            steps.extend([self._parse_timestep(d) for d in dumps])
        self.steps = steps
        self.timesteps = [s["timestep"] for s in self.steps]

    def __len__(self):
        return len(self.timesteps)

    def __getitem__(self, ind):
        return self.steps[ind]

    def _parse_timestep(self, dump):
        step = {}
        lines = dump.split("\n")
        step["timestep"] = int(lines[1])
        step["natoms"] = int(lines[3])
        step["atoms_data"] = np.loadtxt(StringIO("\n".join(lines[9:])),
                                        dtype=self.dtype)
        if self.parse_box:
            box_arr = np.loadtxt(StringIO("\n".join(lines[5:8])))
            bounds = box_arr[:, :2]
            tilt = None
            if "xy xz yz" in lines[4]:
                tilt = box_arr[:, 2]
                x = (0, tilt[0], tilt[1], tilt[0] + tilt[1])
                y = (0, tilt[2])
                bounds -= np.array([[min(x), max(x)], [min(y), max(y)],
                                    [0, 0]])
            step["box"] = LammpsBox(bounds, tilt)
        return step

    def as_dict(self):
        d = {"filename": self.filename}
        json_steps = []
        for step in self.steps:
            json_step = {"timestep": step["timestep"],
                         "natoms": step["natoms"]}
            json_step["atoms_data"] = step["atoms_data"].tolist()
            if self.parse_box:
                json_step["box"] = step["box"].as_dict()
            json_steps.append(json_step)
        d["steps"] = json_steps
        return d

